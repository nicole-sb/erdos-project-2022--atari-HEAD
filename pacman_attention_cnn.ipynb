{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicole-sb/erdos-project-2022--atari-HEAD/blob/main/pacman_attention_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJZ7nNC89hB",
        "outputId": "f21aa4be-2e3f-46e0-f173-17e1bf8d1b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EeYHyjcjpp6A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XKhRJ2Vcclte"
      },
      "outputs": [],
      "source": [
        "parent_path = \"/content/drive/MyDrive/ErdosBootcampProject\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TuYsadRIplNO"
      },
      "outputs": [],
      "source": [
        "train_path = \"{}/raw_data/highscore\".format(parent_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_64I_SeczXfg",
        "outputId": "a128d10b-d255-4022-98d4-1fdc9ed225b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ErdosBootcampProject/raw_data/highscore/118_RZ_4303947_Sep-01-17-15-39.tar.bz2',\n",
              " '/content/drive/MyDrive/ErdosBootcampProject/raw_data/highscore/593_RZ_5037271_Aug-05-15-35-12.tar.bz2']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tars = glob.glob(\"{}/*.tar.bz2\".format(train_path)) #get directories of tar files\n",
        "tars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "al_5EQEeza4W"
      },
      "outputs": [],
      "source": [
        "!tar xjf {tars[0]} #untar first trial '118_RZ_4303947"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X2ox5XlE9pOx"
      },
      "outputs": [],
      "source": [
        "meta_data = pd.read_csv(\"{}/raw_data/combined.csv\".format(parent_path))\n",
        "prefix = \"RZ_4303947_\"\n",
        "meta_data = meta_data[meta_data['frame_id'].str.contains(prefix)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1JVqg7pDFSQS"
      },
      "outputs": [],
      "source": [
        "#Filter rows that have gaze positions outside of bounds (x > 161 and y > 211)\n",
        "meta_data = meta_data[(meta_data['gaze_position_x'] <= 161)]\n",
        "meta_data = meta_data[(meta_data['gaze_position_y'] <= 211)]\n",
        "data_len = int(meta_data.tail(1)['frame_id'].values.tolist()[0].split('_')[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wHl2VGnH9tBi"
      },
      "outputs": [],
      "source": [
        "data_len = int(meta_data.tail(1)['frame_id'].values.tolist()[0].split('_')[-1])\n",
        "tar_name = tars[0].split('/')[-1].split('.tar')[0]\n",
        "\n",
        "def load_img(index):\n",
        "  return Image.open(\"{}/{}{}.png\".format(tar_name, prefix, index+1)).convert('RGB')\n",
        "\n",
        "#Read entire dataset into memory\n",
        "images = [np.array(load_img(i)) for i in range(data_len)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VxXhUDdlCUYs"
      },
      "outputs": [],
      "source": [
        "with open('{}/processed_data/images.pkl'.format(parent_path), 'wb') as f:\n",
        "  pickle.dump(images, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iE5WqZZ-bc1r"
      },
      "outputs": [],
      "source": [
        "with open('{}/processed_data/images.pkl'.format(parent_path), 'rb') as f:\n",
        "  images = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "AiQOiqmgRGzS",
        "outputId": "8e2ce63d-723e-4ea7-b2da-8e2d9cea3945"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       frame_id  score  duration  unclipped_reward  action_int  \\\n",
              "0  RZ_4303947_1    NaN      2817                 0           4   \n",
              "1  RZ_4303947_1    NaN      2817                 0           4   \n",
              "2  RZ_4303947_1    NaN      2817                 0           4   \n",
              "3  RZ_4303947_1    NaN      2817                 0           4   \n",
              "4  RZ_4303947_1    NaN      2817                 0           4   \n",
              "5  RZ_4303947_1    NaN      2817                 0           4   \n",
              "6  RZ_4303947_1    NaN      2817                 0           4   \n",
              "7  RZ_4303947_1    NaN      2817                 0           4   \n",
              "8  RZ_4303947_1    NaN      2817                 0           4   \n",
              "9  RZ_4303947_1    NaN      2817                 0           4   \n",
              "\n",
              "   gaze_position_x  gaze_position_y     action_str        trial_id  \n",
              "0            79.81           118.15  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "1            79.69           119.38  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "2            79.67           120.15  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "3            79.64           121.30  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "4            79.60           121.65  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "5            79.54           122.15  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "6            79.46           122.55  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "7            79.40           122.95  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "8            79.36           123.33  PLAYER_A_LEFT  118_RZ_4303947  \n",
              "9            79.38           123.53  PLAYER_A_LEFT  118_RZ_4303947  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94596674-d66c-488a-9174-7d2548d3427e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frame_id</th>\n",
              "      <th>score</th>\n",
              "      <th>duration</th>\n",
              "      <th>unclipped_reward</th>\n",
              "      <th>action_int</th>\n",
              "      <th>gaze_position_x</th>\n",
              "      <th>gaze_position_y</th>\n",
              "      <th>action_str</th>\n",
              "      <th>trial_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.81</td>\n",
              "      <td>118.15</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.69</td>\n",
              "      <td>119.38</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.67</td>\n",
              "      <td>120.15</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.64</td>\n",
              "      <td>121.30</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.60</td>\n",
              "      <td>121.65</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.54</td>\n",
              "      <td>122.15</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.46</td>\n",
              "      <td>122.55</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.40</td>\n",
              "      <td>122.95</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.36</td>\n",
              "      <td>123.33</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RZ_4303947_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2817</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>79.38</td>\n",
              "      <td>123.53</td>\n",
              "      <td>PLAYER_A_LEFT</td>\n",
              "      <td>118_RZ_4303947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94596674-d66c-488a-9174-7d2548d3427e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94596674-d66c-488a-9174-7d2548d3427e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94596674-d66c-488a-9174-7d2548d3427e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUmlgDeebsMc"
      },
      "source": [
        "# Creating training data and binning of gaze coordinates with only one pass through dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60sA3rH8PS3m",
        "outputId": "253e28f3-21f2-464e-8ed6-a1d2a72d8d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 879295/879295 [02:57<00:00, 4961.94it/s]\n"
          ]
        }
      ],
      "source": [
        "training_data = []\n",
        "prev_frame = meta_data.iloc[0]\n",
        "agg_list = []\n",
        "threshold = 10\n",
        "for curr_index, _ in enumerate(tqdm(range(len(meta_data)))) :\n",
        "  curr_frame = meta_data.iloc[curr_index]\n",
        "  curr_gaze_coords = (curr_frame['gaze_position_x'], curr_frame['gaze_position_y'] )\n",
        "  #First check if new frame, if so then must bin the rest that we left on\n",
        "  if curr_frame['frame_id'] != prev_frame['frame_id']:\n",
        "    if len(agg_list) > 0:\n",
        "      #Average gaze coords for aggregated list of rows\n",
        "      avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "      #Make the training example and add it to training data\n",
        "      example = (int(prev_frame['frame_id'].split('_')[-1])-1, avg_gaze_coords[0], avg_gaze_coords[1], int(prev_frame['action_int']) )\n",
        "      training_data.append(example)\n",
        "      #reset and add the gaze coords for the new frame\n",
        "      agg_list = []\n",
        "    agg_list.append(curr_gaze_coords)\n",
        "    prev_frame = curr_frame\n",
        "    continue\n",
        "\n",
        "  if len(agg_list)+1 == threshold:\n",
        "    #First add the current frame's gaze coords before averaging\n",
        "    agg_list.append(curr_gaze_coords )\n",
        "    #Average the current bin's gaze values\n",
        "    avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "    #Make training example and add to training, then reset\n",
        "    example = (int(curr_frame['frame_id'].split('_')[-1])-1, avg_gaze_coords[0], avg_gaze_coords[1], int(curr_frame['action_int']) )\n",
        "    training_data.append(example)\n",
        "    agg_list = []\n",
        "    prev_frame = curr_frame\n",
        "    continue\n",
        "  \n",
        "  #Otherwise we're in the same frame and don't need to bin yet, so we just add to agg_list and update prev\n",
        "  agg_list.append(curr_gaze_coords)\n",
        "  prev_frame = curr_frame\n",
        "\n",
        "if len(agg_list) > 0:\n",
        "  #Still have left over in agg_list after going through entire meta_data\n",
        "  #Average gaze coords for aggregated list of rows\n",
        "  avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "  #Make the training example and add it to training data\n",
        "  example = (data_len - 1, avg_gaze_coords[0], avg_gaze_coords[1], int(meta_data.tail(1)['action_int']) )\n",
        "  training_data.append(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ghJfIa-qb0yX"
      },
      "outputs": [],
      "source": [
        "with open('{}/processed_data/X_train.pkl'.format(parent_path), 'wb') as f:\n",
        "  pickle.dump(training_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "7ttD6W2iTXZb",
        "outputId": "e0c8d743-2af6-4b30-ce15-1d56c18635e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef get_gaze_list(data, frame_index, threshold=10):\\n  sub_df = data.query(\"`frame_id` == \\'{}{}\\'\".format(prefix, frame_index+1))\\n  sub_df = sub_df.groupby(np.arange(len(sub_df))//threshold).mean()\\n  gaze_tups = list(zip(sub_df.gaze_position_x, sub_df.gaze_position_y))\\n  rep_img = [frame_index for _ in range(len(gaze_tups))]\\n  return zip(rep_img, gaze_tups)\\n\\n#get_gaze_list(meta_data, 0)\\ngaze_dict = { str(frame_index) : get_gaze_list(frame_index) for frame_index in range(data_len)}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\"\"\"\n",
        "def get_gaze_list(data, frame_index, threshold=10):\n",
        "  sub_df = data.query(\"`frame_id` == '{}{}'\".format(prefix, frame_index+1))\n",
        "  sub_df = sub_df.groupby(np.arange(len(sub_df))//threshold).mean()\n",
        "  gaze_tups = list(zip(sub_df.gaze_position_x, sub_df.gaze_position_y))\n",
        "  rep_img = [frame_index for _ in range(len(gaze_tups))]\n",
        "  return zip(rep_img, gaze_tups)\n",
        "\n",
        "#get_gaze_list(meta_data, 0)\n",
        "gaze_dict = { str(frame_index) : get_gaze_list(frame_index) for frame_index in range(data_len)}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VTwcyJ6g60YG"
      },
      "outputs": [],
      "source": [
        "class ErdosDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_path, train_path):\n",
        "        # load pickle serializations for data grabbing\n",
        "        with open(image_path, 'rb') as f:\n",
        "          self.images = pickle.load(f)\n",
        "\n",
        "        with open(train_path, 'rb') as f:\n",
        "          self.X_train = pickle.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        # we will return the number of bins\n",
        "        return len(self.X_train)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "      \"\"\"\n",
        "      Returns the training example for the given index.\n",
        "      Args:\n",
        "      - index (int): index of the example to grab\n",
        "      Returns:\n",
        "      - img (numpy arr): Frame image with shape (210, 160, 3)\n",
        "      - gaze_x (float): average x-coordinate for the given bin\n",
        "      - gaze_y (float): average y-coordinate for the given bin\n",
        "      - y (int): Integer value of the true class (action)\n",
        "      \"\"\"\n",
        "      img_idx, gaze_x, gaze_y, y = self.X_train[index] #Grab the data\n",
        "\n",
        "      #Grab the image associated with img_idx\n",
        "      img = self.images[img_idx]\n",
        "\n",
        "      return img, gaze_x, gaze_y, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K6cCGswb2uf3"
      },
      "outputs": [],
      "source": [
        "h,w = 210,160\n",
        "hidden_size = 256\n",
        "batch_size = 64\n",
        "to_print = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eldpft95CbbN"
      },
      "outputs": [],
      "source": [
        "train_data = ErdosDataset('{}/processed_data/images.pkl'.format(parent_path), '{}/processed_data/X_train.pkl'.format(parent_path))\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size, drop_last=True, shuffle=True, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JPM4ujTnKfJ"
      },
      "source": [
        "Create Learn Weighted Mask "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cY_NyZZznEJw"
      },
      "outputs": [],
      "source": [
        "class Mask(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.MLP = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=1000, out_features= 64 ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=64, out_features=h*w*1)\n",
        "        )\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z, gaze_bias):\n",
        "      \"\"\"\n",
        "      Given a random Tensor of shape (batch_size, 1000), this makes a learned weighted mask\n",
        "      Args:\n",
        "        - z (Tensor): Our input random vector with shape (batch_size, 1000)\n",
        "        - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (batch_size, h, w)\n",
        "      Returns:\n",
        "        - out (Tensor): Our learned weighted mask, which will be applied to our input image later\n",
        "      \"\"\"\n",
        "      #Start we have z with shape (batch_size, 1000)\n",
        "      #Apply our fully connected layer\n",
        "      out = self.MLP(z) #This should now have a shape of (batch_size, h*w*1)\n",
        "      out = out.view((out.shape[0], 1, h, w)) #Unflatten, so this should now have a shape of (batch_size, h, w, 1)\n",
        "      \n",
        "      #Reshape gaze_biase from (batch_size, h, w) to (batch_size, 1, h, w) to match out shape\n",
        "      gaze_bias = gaze_bias.unsqueeze(1)\n",
        "\n",
        "      #Apply gaze_bias to learned mask\n",
        "      if to_print:\n",
        "        print(\"[Mask] Out shape: \", out.shape)\n",
        "        print(\"[Mask] Gaze Bias shape: \", gaze_bias.shape)\n",
        "      out = out + gaze_bias\n",
        "      #Apply sigmoid now to make sure values go between 0 and 1\n",
        "      return self.sigmoid(out) #This is the learned weight mask with gaze information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZBejCk6g3Z-o"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.learned_mask = Mask()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=100, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.MLP = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=100*h*w, out_features=16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=16, out_features=10)\n",
        "        )\n",
        "        #self.final_activation = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x, z, gaze_bias):\n",
        "    \"\"\"\n",
        "    This applies our input image with the mask and then runs it through the rest of the CNN\n",
        "    Args:\n",
        "      - x (Tensor): Our input image with shape (batch_size, 3, h, w)\n",
        "      - z (Tensor): Random tensor used to make mask with shape (batch_size, 1000)\n",
        "      - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (batch_size, h, w)\n",
        "    Returns:\n",
        "      - prediction: Which action?\n",
        "    \"\"\"\n",
        "\n",
        "    learned_weight_mask = self.learned_mask(z, gaze_bias)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Mask shape: \", learned_weight_mask.shape)\n",
        "\n",
        "    #Apply the mask to the image to get initial input to CNN parts\n",
        "    out = torch.mul(x, learned_weight_mask) #Shape (batch_size, 3,h,w)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Element mult shape: \", out.shape)\n",
        "\n",
        "    #Apply the convs etc.\n",
        "    out = self.conv1(out) #Shape (batch_size, 100, h, w)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Conv1 shape: \", out.shape)\n",
        "    out = self.relu1(out)\n",
        "    out = self.maxpool1(out) #Shape (batch_size, 100, h, w)\n",
        "\n",
        "    #Flatten the image for the fully connected layers\n",
        "    out = self.flatten(out)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Flatten shape: \", out.shape)\n",
        "\n",
        "    #Apply the Fully connected layers\n",
        "    out = self.MLP(out)\n",
        "    if to_print:\n",
        "      print(\"[CNN] MLP shape: \", out.shape)\n",
        "    #out = self.final_activation(out) #This will get you probability vector with probs for each class\n",
        "\n",
        "    #Then output the scores\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "pK__zQu52wcN",
        "outputId": "27a77402-166e-4d44-847b-877dfc3f4c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1411 [00:19<7:39:12, 19.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.572195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 1001/1411 [2:33:36<1:04:56,  9.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.657853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1411/1411 [3:42:18<00:00,  9.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 23.621323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1411 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9c0461b6ef61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Grab the train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgazes_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgazes_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgazes_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgazes_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0;31m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_model = CNN().to(device)\n",
        "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=1e-2, momentum=0.9)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "data_iterator = iter(train_data_loader)\n",
        "\n",
        "#This is training\n",
        "for epoch in range(num_epochs):\n",
        "  correct = 0\n",
        "  for step in tqdm(range(len(data_iterator))):\n",
        "    #Grab the train data\n",
        "    imgs, gazes_x, gazes_y, labels = data_iterator.next()\n",
        "    imgs = imgs.permute((0, 3, 1, 2)).to(device)\n",
        "    gazes_x = gazes_x.long().to(device)\n",
        "    gazes_y = gazes_y.long().to(device)\n",
        "    labels = labels.long().to(device)\n",
        "\n",
        "    if to_print:\n",
        "      print(\"Image shape: \", imgs.shape)\n",
        "      print(\"Gazes_X shape: \", gazes_x.shape)\n",
        "      print(\"Labels shape: \", labels.shape)\n",
        "\n",
        "    # #Make one-hot-encoded gaze_bias\n",
        "    with torch.no_grad():\n",
        "      gaze_bias = torch.zeros((batch_size, h, w), requires_grad=False).to(device)\n",
        "      for b in range(batch_size):\n",
        "        gaze_bias[b, gazes_y[b]-1, gazes_x[b]-1] = 1\n",
        "    if to_print:\n",
        "      print(\"Gaze bias shape: \", gaze_bias.shape)\n",
        "\n",
        "    # #Make random noise that will be used to make mask\n",
        "    z = torch.rand((batch_size, 1000), requires_grad=True).to(device)\n",
        "\n",
        "    # #Pass image into cnn_model\n",
        "    logits = cnn_model(imgs, z, gaze_bias)\n",
        "    if to_print:\n",
        "      print(\"[Train] logits shape: \", logits.shape)\n",
        "    # #Calculate loss \n",
        "    loss = loss_fn(logits, labels)\n",
        "\n",
        "    # #Zero gradients before calculating gradients of loss with respect to the image\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # #Calcualte gradients using backprob\n",
        "    loss.backward()\n",
        "\n",
        "    # #Step optimizer to update weights using the new grads\n",
        "    optimizer.step()\n",
        "    correct += ((torch.argmax(logits) == labels).float().sum()*(1/batch_size))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "            loss  = loss.item()\n",
        "            print(f\"Loss: {loss:>7f}\")\n",
        "  acc = 100 * correct / len(data_iterator)\n",
        "  print(f\"Acc: {acc:>7f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfX3gmXokCXH"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_model = CNN().to(device)\n",
        "print(cnn_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCKwgK1vkXJ0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def test(test_data, cnn_model, loss_fn):\n",
        "    size = len(test_data.dataset)\n",
        "    num_batches = len(test_data)\n",
        "    cnn_model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for image, gaze_coords, y in test_data:\n",
        "            image, gaze_coords, y = image.to(device), gaze_coords.to(device), y.to(device)\n",
        "            y_hat = cnn_model(image, gaze_coords, y)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlHALohro71x"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "cnn_model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0oWaOQSOm0X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pacman_attention_cnn",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}