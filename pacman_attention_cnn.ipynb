{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicole-sb/erdos-project-2022--atari-HEAD/blob/main/pacman_attention_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJZ7nNC89hB",
        "outputId": "f7d8b7c9-39a6-42d9-fe22-3f3081896958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeYHyjcjpp6A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKhRJ2Vcclte"
      },
      "outputs": [],
      "source": [
        "parent_path = \"/content/drive/MyDrive/erdos-project-2022--atari-HEAD\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuYsadRIplNO"
      },
      "outputs": [],
      "source": [
        "train_path = \"{}/final_data/ms_pacman/highscore\".format(parent_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_64I_SeczXfg",
        "outputId": "67841a4b-d637-4b26-c166-5b9af80c5340"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/erdos-project-2022--atari-HEAD/final_data/ms_pacman/highscore/118_RZ_4303947_Sep-01-17-15-39.tar.bz2',\n",
              " '/content/drive/MyDrive/erdos-project-2022--atari-HEAD/final_data/ms_pacman/highscore/593_RZ_5037271_Aug-05-15-35-12.tar.bz2']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tars = glob.glob(\"{}/*.tar.bz2\".format(train_path)) #get directories of tar files\n",
        "tars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al_5EQEeza4W"
      },
      "outputs": [],
      "source": [
        "!tar xjf {tars[0]} #untar first trial '118_RZ\n",
        "!tar xjf {tars[1]} #untar second trial '593_RZ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/erdos-project-2022--atari-HEAD/final_data/ms_pacman/highscore/combined_trial_data.csv.zip -d /content/drive/MyDrive/erdos-project-2022--atari-HEAD/final_data/ms_pacman/highscore/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXedOmVVbCEj",
        "outputId": "61fe96e0-0df4-40da-d466-f31ebb1b95cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/erdos-project-2022--atari-HEAD/final_data/ms_pacman/highscore/combined_trial_data.csv.zip\n",
            "replace /content/drive/MyDrive/erdos-project-2022--atari-HEAD/final_data/ms_pacman/highscore/combined_trial_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2ox5XlE9pOx"
      },
      "outputs": [],
      "source": [
        "trial_text_df = pd.read_csv(\"{}/combined_trial_data.csv\".format(train_path))\n",
        "trial_1 = trial_text_df[trial_text_df['trial_id'] == '118_RZ']\n",
        "trial_2 = trial_text_df[trial_text_df['trial_id'] == '593_RZ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHl2VGnH9tBi"
      },
      "outputs": [],
      "source": [
        "data_len = int(trial_1.tail(1)['frame_id'].values.tolist()[0].split('_')[-1])\n",
        "\n",
        "def load_img(index):\n",
        "  return Image.open(\"./118_RZ_4303947_Sep-01-17-15-39/RZ_4303947_{}.png\".format(index+1)).convert('RGB')\n",
        "\n",
        "#Read entire dataset into memory\n",
        "images_1 = [np.array(load_img(i)) for i in range(data_len)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_len = int(trial_2.tail(1)['frame_id'].values.tolist()[0].split('_')[-1])\n",
        "\n",
        "def load_img(index):\n",
        "  return Image.open(\"./593_RZ_5037271_Aug-05-15-35-12/RZ_5037271_{}.png\".format(index+1)).convert('RGB')\n",
        "\n",
        "#Read entire dataset into memory\n",
        "images_2 = [np.array(load_img(i)) for i in range(data_len)]"
      ],
      "metadata": {
        "id": "EZhGn5UNvWQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_1.extend(images_2)"
      ],
      "metadata": {
        "id": "akFG040u8tGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = images_1"
      ],
      "metadata": {
        "id": "gTYAekSr8y0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxXhUDdlCUYs"
      },
      "outputs": [],
      "source": [
        "with open('{}/processed_data/images.pkl'.format(train_path), 'wb') as f:\n",
        "  pickle.dump(images, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUmlgDeebsMc"
      },
      "source": [
        "# Creating training data and binning of gaze coordinates with only one pass through dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60sA3rH8PS3m"
      },
      "outputs": [],
      "source": [
        "def bin(df_train):\n",
        "  training_data = []\n",
        "  prev_frame = df_train.iloc[0]\n",
        "  agg_list = []\n",
        "  threshold = 10\n",
        "  for curr_index, _ in enumerate(tqdm(range(len(df_train)))) :\n",
        "    curr_frame = df_train.iloc[curr_index]\n",
        "    curr_gaze_coords = (curr_frame['gaze_position_x'], curr_frame['gaze_position_y'] )\n",
        "    #First check if new frame, if so then must bin the rest that we left on\n",
        "    if curr_frame['frame_id'] != prev_frame['frame_id']:\n",
        "      if len(agg_list) > 0:\n",
        "        #Average gaze coords for aggregated list of rows\n",
        "        avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "        #Make the training example and add it to training data\n",
        "        example = (int(prev_frame['frame_id'].split('_')[-1])-1, avg_gaze_coords[0], avg_gaze_coords[1], int(prev_frame['action_int']) )\n",
        "        training_data.append(example)\n",
        "        #reset and add the gaze coords for the new frame\n",
        "        agg_list = []\n",
        "      agg_list.append(curr_gaze_coords)\n",
        "      prev_frame = curr_frame\n",
        "      continue\n",
        "\n",
        "    if len(agg_list)+1 == threshold:\n",
        "      #First add the current frame's gaze coords before averaging\n",
        "      agg_list.append(curr_gaze_coords )\n",
        "      #Average the current bin's gaze values\n",
        "      avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "      #Make training example and add to training, then reset\n",
        "      example = (int(curr_frame['frame_id'].split('_')[-1])-1, avg_gaze_coords[0], avg_gaze_coords[1], int(curr_frame['action_int']) )\n",
        "      training_data.append(example)\n",
        "      agg_list = []\n",
        "      prev_frame = curr_frame\n",
        "      continue\n",
        "    \n",
        "    #Otherwise we're in the same frame and don't need to bin yet, so we just add to agg_list and update prev\n",
        "    agg_list.append(curr_gaze_coords)\n",
        "    prev_frame = curr_frame\n",
        "\n",
        "  if len(agg_list) > 0:\n",
        "    #Still have left over in agg_list after going through entire df_train\n",
        "    #Average gaze coords for aggregated list of rows\n",
        "    avg_gaze_coords = tuple(sum(y) / len(y) for y in zip(*agg_list))\n",
        "    #Make the training example and add it to training data\n",
        "    example = (data_len - 1, avg_gaze_coords[0], avg_gaze_coords[1], int(df_train.tail(1)['action_int']) )\n",
        "    training_data.append(example)\n",
        "  return training_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binned_data_1 = bin(trial_1)\n",
        "binned_data_2 = bin(trial_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuPDkip1wslb",
        "outputId": "8feeee04-c316-492d-e9e6-94419a98a8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 882316/882316 [02:17<00:00, 6396.11it/s]\n",
            "100%|██████████| 881897/881897 [01:57<00:00, 7533.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train and test\n",
        "# first 80% of frames for training, last 20% for testing\n",
        "\n",
        "def train_test_split(binned_data, ratio = 0.8):\n",
        "  total_len = len(binned_data)\n",
        "  split = int(round(total_len*ratio, 0))\n",
        "  train = binned_data[:split]\n",
        "  test = binned_data[split:]\n",
        "  return train, test\n",
        "\n",
        "train_1, test_1 = train_test_split(binned_data_1)\n",
        "train_2, test_2 = train_test_split(binned_data_2)\n",
        "\n",
        "X_train = train_1 + train_2\n",
        "X_test = test_1 + test_2\n",
        "\n",
        "with open('{}/processed_data/X_train.pkl'.format(train_path), 'wb') as f:\n",
        "  pickle.dump(X_train, f)\n",
        "\n",
        "with open('{}/processed_data/X_test.pkl'.format(train_path), 'wb') as f:\n",
        "  pickle.dump(X_test, f)\n",
        "\n",
        "# trial 1\n",
        "# total_gaze_1 = len(binned_data)\n",
        "# train_len_1  = int(round(total_gaze_1*0.8, 0))\n",
        "\n",
        "# df_train_1 = trial_1[:train_len_1]\n",
        "# df_test_1 = trial_1[train_len_1:]\n",
        "\n",
        "# # trial 2\n",
        "# total_gaze_2 = len(trial_2)\n",
        "# train_len_2 = int(round(total_gaze_2*0.8, 0))\n",
        "\n",
        "# df_train_2 = trial_2[:train_len_2]\n",
        "# df_test_2 = trial_2[train_len_2:]\n",
        "\n",
        "# # combine trial 1+2 train and save\n",
        "# df_train = pd.concat([df_train_1, df_train_2])\n",
        "\n",
        "# # combine trial 1+2 test and save\n",
        "# df_test = pd.concat([df_test_1, df_test_2])\n",
        "\n",
        "# df_train.to_csv(\"{}/train_data.csv\".format(train_path), index=False)\n",
        "# df_test.to_csv(\"{}/test_data.csv\".format(train_path), index=False)"
      ],
      "metadata": {
        "id": "BkU1K6og6Soo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTwcyJ6g60YG"
      },
      "outputs": [],
      "source": [
        "class ErdosDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, train_path, images):\n",
        "        # load pickle serializations for data grabbing\n",
        "        self.images = images\n",
        "        \n",
        "        with open(train_path, 'rb') as f:\n",
        "          self.data = pickle.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        # we will return the number of bins\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "      \"\"\"\n",
        "      Returns the training example for the given index.\n",
        "      Args:\n",
        "      - index (int): index of the example to grab\n",
        "      Returns:\n",
        "      - img (numpy arr): Frame image with shape (210, 160, 3)\n",
        "      - gaze_x (float): average x-coordinate for the given bin\n",
        "      - gaze_y (float): average y-coordinate for the given bin\n",
        "      - y (int): Integer value of the true class (action)\n",
        "      \"\"\"\n",
        "      img_idx, gaze_x, gaze_y, y = self.data[index] #Grab the data\n",
        "\n",
        "      #Grab the image associated with img_idx\n",
        "      img = self.images[img_idx]\n",
        "\n",
        "      return img, gaze_x, gaze_y, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '{}/processed_data/images.pkl'.format(train_path)\n",
        "with open(image_path, 'rb') as f:\n",
        "  images = pickle.load(f)"
      ],
      "metadata": {
        "id": "VwTp_1i7BYn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6cCGswb2uf3"
      },
      "outputs": [],
      "source": [
        "h,w = 210,160\n",
        "batch_size = 512\n",
        "to_print = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eldpft95CbbN"
      },
      "outputs": [],
      "source": [
        "train_data = ErdosDataset('{}/processed_data/X_train.pkl'.format(train_path), images)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size, drop_last=True, shuffle=True, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = ErdosDataset('{}/processed_data/X_test.pkl'.format(train_path), images)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size, drop_last=True, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "U4tlVQFezbn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JPM4ujTnKfJ"
      },
      "source": [
        "Create Learn Weighted Mask "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY_NyZZznEJw"
      },
      "outputs": [],
      "source": [
        "class Mask(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.MLP = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=1000, out_features= 64 ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=64, out_features=h*w*1)\n",
        "        )\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z, gaze_bias):\n",
        "      \"\"\"\n",
        "      Given a random Tensor of shape (batch_size, 1000), this makes a learned weighted mask\n",
        "      Args:\n",
        "        - z (Tensor): Our input random vector with shape (batch_size, 1000)\n",
        "        - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (batch_size, h, w)\n",
        "      Returns:\n",
        "        - out (Tensor): Our learned weighted mask, which will be applied to our input image later\n",
        "      \"\"\"\n",
        "      #Start we have z with shape (batch_size, 1000)\n",
        "      #Apply our fully connected layer\n",
        "      out = self.MLP(z) #This should now have a shape of (batch_size, h*w*1)\n",
        "      out = out.view((out.shape[0], 1, h, w)) #Unflatten, so this should now have a shape of (batch_size, h, w, 1)\n",
        "      \n",
        "      #Reshape gaze_biase from (batch_size, h, w) to (batch_size, 1, h, w) to match out shape\n",
        "      gaze_bias = gaze_bias.unsqueeze(1)\n",
        "\n",
        "      #Apply gaze_bias to learned mask\n",
        "      if to_print:\n",
        "        print(\"[Mask] Out shape: \", out.shape)\n",
        "        print(\"[Mask] Gaze Bias shape: \", gaze_bias.shape)\n",
        "      out = out + gaze_bias\n",
        "      #Apply sigmoid now to make sure values go between 0 and 1\n",
        "      return self.sigmoid(out) #This is the learned weight mask with gaze information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBejCk6g3Z-o"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.learned_mask = Mask()\n",
        "\n",
        "        self.convs = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=7, stride=4, padding=1),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU(),\n",
        "            # torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
        "            # torch.nn.BatchNorm2d(16),\n",
        "            # torch.nn.ReLU(),\n",
        "            # torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "            # torch.nn.BatchNorm2d(16),\n",
        "            # torch.nn.ReLU(),\n",
        "            # torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "            # torch.nn.BatchNorm2d(16),\n",
        "            # torch.nn.ReLU(),\n",
        "            # torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            # torch.nn.BatchNorm2d(32),\n",
        "            # torch.nn.ReLU(),\n",
        "            # torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            # torch.nn.BatchNorm2d(32),\n",
        "            # torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # #conv block 1\n",
        "        # self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4, padding=1)\n",
        "        # self.relu1 = torch.nn.ReLU()\n",
        "\n",
        "        # #conv block 2\n",
        "        # self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        # self.relu2 = torch.nn.ReLU()\n",
        "        \n",
        "        # #conv block 3\n",
        "        # self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        # self.relu3 = torch.nn.ReLU()\n",
        "\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.MLP = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=4160, out_features=32),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=32, out_features=10)\n",
        "        )\n",
        "        #self.final_activation = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x, z, gaze_bias):\n",
        "    \"\"\"\n",
        "    This applies our input image with the mask and then runs it through the rest of the CNN\n",
        "    Args:\n",
        "      - x (Tensor): Our input image with shape (batch_size, 3, h, w)\n",
        "      - z (Tensor): Random tensor used to make mask with shape (batch_size, 1000)\n",
        "      - gaze_bias (Tensor): Our one-hot-encoded tensor with shape (batch_size, h, w)\n",
        "    Returns:\n",
        "      - prediction: Which action?\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    #print(\"[CNN] X shape: \", out.shape)\n",
        "    # learned_weight_mask = self.learned_mask(z, gaze_bias)\n",
        "    # if to_print:\n",
        "    #   print(\"[CNN] Mask shape: \", learned_weight_mask.shape)\n",
        "\n",
        "    # #Apply the mask to the image to get initial input to CNN parts\n",
        "    # out = torch.mul(x, learned_weight_mask) #Shape (batch_size, 3,h,w)\n",
        "    # if to_print:\n",
        "    #   print(\"[CNN] Element mult shape: \", out.shape)\n",
        "\n",
        "    #Apply the convs etc.\n",
        "    out = self.convs(out)\n",
        "    # out = self.conv1(out) #Shape (batch_size, 100, h, w)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Conv1 shape: \", out.shape)\n",
        "    # out = self.relu1(out)\n",
        "\n",
        "    # out = self.conv2(out) #Shape (batch_size, 100, h, w)\n",
        "    # if to_print:\n",
        "    #   print(\"[CNN] Conv2 shape: \", out.shape)\n",
        "    # out = self.relu2(out)\n",
        "\n",
        "    # out = self.conv3(out) #Shape (batch_size, 100, h, w)\n",
        "    # if to_print:\n",
        "    #   print(\"[CNN] Conv3 shape: \", out.shape)\n",
        "    # out = self.relu3(out)\n",
        "\n",
        "    #Flatten the image for the fully connected layers\n",
        "    out = self.flatten(out)\n",
        "    if to_print:\n",
        "      print(\"[CNN] Flatten shape: \", out.shape)\n",
        "\n",
        "    #Apply the Fully connected layers\n",
        "    out = self.MLP(out)\n",
        "    if to_print:\n",
        "      print(\"[CNN] MLP shape: \", out.shape)\n",
        "    #out = self.final_activation(out) #This will get you probability vector with probs for each class\n",
        "\n",
        "    #Then output the scores\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, loss_fn, optimizer, batch_size = 64):\n",
        "  total_correct = 0\n",
        "  total_loss = 0\n",
        "  total_imgs = 0\n",
        "  for step, values in enumerate(tqdm(data_loader)):\n",
        "    #Grab the train data\n",
        "    imgs, gazes_x, gazes_y, labels = values\n",
        "    imgs = imgs.permute((0, 3, 1, 2)).float().to(device)\n",
        "    gazes_x = gazes_x.long().to(device)\n",
        "    gazes_y = gazes_y.long().to(device)\n",
        "    labels = labels.long().to(device)\n",
        "\n",
        "    if to_print:\n",
        "      print(\"Image shape: \", imgs.shape)\n",
        "      print(\"Gazes_X shape: \", gazes_x.shape)\n",
        "      print(\"Labels shape: \", labels.shape)\n",
        "\n",
        "    # #Make one-hot-encoded gaze_bias\n",
        "    with torch.no_grad():\n",
        "      gaze_bias = torch.zeros((batch_size, h, w), requires_grad=False).to(device)\n",
        "      for b in range(batch_size):\n",
        "        gaze_bias[b, gazes_y[b]-1, gazes_x[b]-1] = 1\n",
        "    if to_print:\n",
        "      print(\"Gaze bias shape: \", gaze_bias.shape)\n",
        "\n",
        "    # #Make random noise that will be used to make mask\n",
        "    #z = torch.rand((batch_size, 1000), requires_grad=True).to(device)\n",
        "\n",
        "    # #Pass image into cnn_model\n",
        "    with torch.cuda.amp.autocast():\n",
        "      logits = cnn_model(imgs, None, gaze_bias)\n",
        "    if to_print:\n",
        "      print(\"[Train] logits shape: \", logits.shape)\n",
        "    # #Calculate loss \n",
        "    with torch.cuda.amp.autocast():\n",
        "      loss = loss_fn(logits, labels)\n",
        "\n",
        "    # #Zero gradients before calculating gradients of loss with respect to the image\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # #Calcualte gradients using backprob\n",
        "    loss.backward()\n",
        "\n",
        "    # #Step optimizer to update weights using the new grads\n",
        "    optimizer.step()\n",
        "    total_correct += (torch.argmax(logits, dim = 1) == labels).float().sum()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_imgs += batch_size\n",
        "\n",
        "  total_loss /= batch_size\n",
        "  total_correct /= total_imgs\n",
        "  return total_correct, total_loss"
      ],
      "metadata": {
        "id": "F74igx4pb2qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_data, cnn_model, loss_fn):\n",
        "    #size = len(test_data.dataset)\n",
        "    cnn_model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    total_imgs = 0\n",
        "    with torch.no_grad():\n",
        "        for step, values in enumerate(tqdm(test_data)):\n",
        "            image, gaze_x, gaze_y, y = values\n",
        "            image, gaze_x, gaze_y, y = image.permute((0, 3, 1, 2)).float().to(device), gaze_x.long().to(device), gaze_y.long().to(device), y.long().to(device)\n",
        "            y_hat = cnn_model(image, None, y)\n",
        "            test_loss += loss_fn(y_hat, y).item()\n",
        "            correct += (y_hat.argmax(1) == y).type(torch.float).sum().item()\n",
        "            total_imgs+=batch_size\n",
        "    test_loss /= batch_size\n",
        "    correct /= total_imgs\n",
        "    return correct, test_loss"
      ],
      "metadata": {
        "id": "QTDZZMeOdR_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK__zQu52wcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6829a24a-23dc-4074-af4a-f9bc851cdbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.04it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Acc: 25.605192, Loss: 0.988807\n",
            "Test Acc: 25.022007, Test Loss: 0.233251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.01it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Acc: 33.525665, Loss: 0.903018\n",
            "Test Acc: 28.036972, Test Loss: 0.238552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.02it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Acc: 42.516918, Loss: 0.825277\n",
            "Test Acc: 28.270797, Test Loss: 0.290207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  6.98it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Acc: 49.172672, Loss: 0.738977\n",
            "Test Acc: 28.391835, Test Loss: 0.350520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.04it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Acc: 53.389084, Loss: 0.663694\n",
            "Test Acc: 28.251540, Test Loss: 0.403950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.01it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Acc: 56.115894, Loss: 0.601484\n",
            "Test Acc: 27.462038, Test Loss: 0.441829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.10it/s]\n",
            "100%|██████████| 71/71 [00:05<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Acc: 57.608932, Loss: 0.557949\n",
            "Test Acc: 27.937940, Test Loss: 0.550282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.07it/s]\n",
            "100%|██████████| 71/71 [00:05<00:00, 11.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Acc: 58.438324, Loss: 0.527481\n",
            "Test Acc: 27.869168, Test Loss: 0.545614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.02it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Acc: 58.941734, Loss: 0.507401\n",
            "Test Acc: 27.445533, Test Loss: 0.592916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.01it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Acc: 59.273903, Loss: 0.489130\n",
            "Test Acc: 28.642165, Test Loss: 0.588025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.06it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Acc: 59.518730, Loss: 0.473902\n",
            "Test Acc: 28.023217, Test Loss: 0.579083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.05it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Acc: 59.810329, Loss: 0.461628\n",
            "Test Acc: 28.133253, Test Loss: 0.577285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.02it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Acc: 59.987759, Loss: 0.450051\n",
            "Test Acc: 28.130502, Test Loss: 0.643221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.12it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Acc: 60.027649, Loss: 0.440068\n",
            "Test Acc: 28.069982, Test Loss: 0.648651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.03it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Acc: 60.103298, Loss: 0.435389\n",
            "Test Acc: 28.790713, Test Loss: 0.625796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.03it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, Acc: 60.131489, Loss: 0.427171\n",
            "Test Acc: 27.797645, Test Loss: 0.619957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.11it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16, Acc: 60.373569, Loss: 0.422246\n",
            "Test Acc: 28.529379, Test Loss: 0.629300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.12it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17, Acc: 60.284851, Loss: 0.415480\n",
            "Test Acc: 28.204776, Test Loss: 0.651899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.13it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18, Acc: 60.477413, Loss: 0.410020\n",
            "Test Acc: 28.025968, Test Loss: 0.655806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.11it/s]\n",
            "100%|██████████| 71/71 [00:05<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19, Acc: 60.498734, Loss: 0.407393\n",
            "Test Acc: 28.039723, Test Loss: 0.671041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.12it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Acc: 60.557194, Loss: 0.403155\n",
            "Test Acc: 28.325814, Test Loss: 0.650775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.13it/s]\n",
            "100%|██████████| 71/71 [00:05<00:00, 11.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21, Acc: 60.575069, Loss: 0.398621\n",
            "Test Acc: 28.331316, Test Loss: 0.656347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.11it/s]\n",
            "100%|██████████| 71/71 [00:05<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22, Acc: 60.374947, Loss: 0.396281\n",
            "Test Acc: 28.047975, Test Loss: 0.651595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.09it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23, Acc: 60.621834, Loss: 0.393106\n",
            "Test Acc: 28.801717, Test Loss: 0.710221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.04it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24, Acc: 60.715366, Loss: 0.388995\n",
            "Test Acc: 28.639415, Test Loss: 0.635867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.08it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25, Acc: 60.534496, Loss: 0.388622\n",
            "Test Acc: 27.926937, Test Loss: 0.677933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.07it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26, Acc: 60.595703, Loss: 0.386060\n",
            "Test Acc: 28.383583, Test Loss: 0.695243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.12it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27, Acc: 60.686481, Loss: 0.383455\n",
            "Test Acc: 28.433099, Test Loss: 0.681532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.04it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 28, Acc: 60.641777, Loss: 0.380843\n",
            "Test Acc: 28.097491, Test Loss: 0.678971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.03it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 29, Acc: 60.810268, Loss: 0.380617\n",
            "Test Acc: 28.694432, Test Loss: 0.624050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  6.99it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30, Acc: 60.762131, Loss: 0.378190\n",
            "Test Acc: 28.543134, Test Loss: 0.671434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.05it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31, Acc: 60.761440, Loss: 0.375931\n",
            "Test Acc: 27.695863, Test Loss: 0.704699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.09it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 32, Acc: 60.882481, Loss: 0.373469\n",
            "Test Acc: 28.364327, Test Loss: 0.703534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.16it/s]\n",
            "100%|██████████| 71/71 [00:05<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 33, Acc: 60.784824, Loss: 0.372567\n",
            "Test Acc: 28.911752, Test Loss: 0.723709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.14it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 34, Acc: 60.925121, Loss: 0.370689\n",
            "Test Acc: 28.113996, Test Loss: 0.690098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.11it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 35, Acc: 60.777260, Loss: 0.369679\n",
            "Test Acc: 27.396017, Test Loss: 0.699701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.13it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36, Acc: 60.915493, Loss: 0.368994\n",
            "Test Acc: 28.292804, Test Loss: 0.657808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  6.98it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 37, Acc: 60.877670, Loss: 0.367609\n",
            "Test Acc: 28.430348, Test Loss: 0.673623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.03it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 38, Acc: 60.861851, Loss: 0.366739\n",
            "Test Acc: 28.273548, Test Loss: 0.732515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:39<00:00,  7.11it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 39, Acc: 61.039280, Loss: 0.364876\n",
            "Test Acc: 27.398768, Test Loss: 0.624488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.10it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40, Acc: 60.844654, Loss: 0.363899\n",
            "Test Acc: 28.433099, Test Loss: 0.687483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.10it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 41, Acc: 60.820587, Loss: 0.364082\n",
            "Test Acc: 28.064481, Test Loss: 0.727643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.09it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 42, Acc: 60.863228, Loss: 0.362208\n",
            "Test Acc: 28.323063, Test Loss: 0.703512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 284/284 [00:40<00:00,  7.09it/s]\n",
            "100%|██████████| 71/71 [00:06<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 43, Acc: 60.814400, Loss: 0.362113\n",
            "Test Acc: 27.536312, Test Loss: 0.704366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 134/284 [00:18<00:20,  7.25it/s]"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_model = CNN().to(device)\n",
        "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=1e-3, momentum=0.9)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "#optimizer.lr = 'blah'\n",
        "\n",
        "#This is training\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "  correct, total_loss = train(cnn_model, train_data_loader, loss_fn, optimizer, batch_size)\n",
        "  test_correct, test_loss = test(test_data_loader, cnn_model, loss_fn)\n",
        "  acc = 100.0 * correct\n",
        "  print(f\"Epoch: {epoch}, Acc: {acc:>7f}, Loss: {total_loss:>7f}\")\n",
        "  test_acc = test_correct*100.0\n",
        "  print(f\"Test Acc: {test_acc:>7f}, Test Loss: {test_loss:>7f}\")\n",
        "  losses.append(total_loss)\n",
        "  test_losses.append(test_loss)\n",
        "#Graph epoch and loss:\n",
        "plt.plot(np.array(losses), 'r')\n",
        "plt.plot(np.array(test_losses), 'b')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfX3gmXokCXH"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_model = CNN().to(device)\n",
        "print(cnn_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlHALohro71x"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "cnn_model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0oWaOQSOm0X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pacman_attention_cnn",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}